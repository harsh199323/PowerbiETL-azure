{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1402f0a-1727-4a84-85ba-ecf0be463333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-138910747600774>, line 9\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m configs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.auth.type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAuth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth.provider.type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6e46b03e-36b4-42fb-99bd-de95d242ae4d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.secret\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr58Q~UeBQV-BFRLHLpkyORetpai8uuyp-Kw7doE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://login.microsoftonline.com/b327953b-b3ba-44f0-82e1-1cea0a1cf2ef/oauth2/token\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#raw data = bronze data\u001b[39;00m\n",
       "\u001b[0;32m----> 9\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n",
       "\u001b[1;32m     10\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://bronze-data@datalakeamazonsales.dfs.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# contrainer@storageacc\u001b[39;00m\n",
       "\u001b[1;32m     11\u001b[0m mount_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/bronze-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     12\u001b[0m extra_configs \u001b[38;5;241m=\u001b[39m configs)\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#transformed data\u001b[39;00m\n",
       "\u001b[1;32m     16\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n",
       "\u001b[1;32m     17\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://transformed-data@datalakeamazonsales.dfs.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# contrainer@storageacc\u001b[39;00m\n",
       "\u001b[1;32m     18\u001b[0m mount_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/transformed-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     19\u001b[0m extra_configs \u001b[38;5;241m=\u001b[39m configs)\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
       "\n",
       "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o405.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:375)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:375)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:319)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:402)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:402)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:35)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:377)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:157)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n",
       "\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:81)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:76)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\nFile \u001b[0;32m<command-138910747600774>, line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m configs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.auth.type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAuth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth.provider.type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6e46b03e-36b4-42fb-99bd-de95d242ae4d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.secret\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr58Q~UeBQV-BFRLHLpkyORetpai8uuyp-Kw7doE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.oauth2.client.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://login.microsoftonline.com/b327953b-b3ba-44f0-82e1-1cea0a1cf2ef/oauth2/token\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#raw data = bronze data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m     10\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://bronze-data@datalakeamazonsales.dfs.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# contrainer@storageacc\u001b[39;00m\n\u001b[1;32m     11\u001b[0m mount_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/bronze-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m extra_configs \u001b[38;5;241m=\u001b[39m configs)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#transformed data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m     17\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://transformed-data@datalakeamazonsales.dfs.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# contrainer@storageacc\u001b[39;00m\n\u001b[1;32m     18\u001b[0m mount_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/transformed-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m extra_configs \u001b[38;5;241m=\u001b[39m configs)\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n\n\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o405.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:375)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:375)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:319)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:402)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:402)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:35)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:377)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:157)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:81)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:76)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-data; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"client ID\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'Secret Key',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/Directory (tenant) ID/oauth2/token\"}\n",
    "\n",
    "\n",
    "#raw data = bronze data\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://bronze-data@datalakeamazonsales.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/bronze-data\",\n",
    "extra_configs = configs)\n",
    "\n",
    "\n",
    "#transformed data\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://transformed-data@datalakeamazonsales.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/transformed-data\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6aa704-8cf8-4355-8958-33fbdf1982ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/raw-data/admissions_hospital</td><td>admissions_hospital</td><td>1049562</td><td>1697563433000</td></tr><tr><td>dbfs:/mnt/raw-data/death_uk_ind_only</td><td>death_uk_ind_only</td><td>133445</td><td>1697563390000</td></tr><tr><td>dbfs:/mnt/raw-data/death_world</td><td>death_world</td><td>14315827</td><td>1697563405000</td></tr><tr><td>dbfs:/mnt/raw-data/response_country</td><td>response_country</td><td>46403</td><td>1697563420000</td></tr><tr><td>dbfs:/mnt/raw-data/test</td><td>test</td><td>84819</td><td>1697563446000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/raw-data/admissions_hospital",
         "admissions_hospital",
         1049562,
         1697563433000
        ],
        [
         "dbfs:/mnt/raw-data/death_uk_ind_only",
         "death_uk_ind_only",
         133445,
         1697563390000
        ],
        [
         "dbfs:/mnt/raw-data/death_world",
         "death_world",
         14315827,
         1697563405000
        ],
        [
         "dbfs:/mnt/raw-data/response_country",
         "response_country",
         46403,
         1697563420000
        ],
        [
         "dbfs:/mnt/raw-data/test",
         "test",
         84819,
         1697563446000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%fs\n",
    "ls \"/mnt/raw-data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fd7950-9d1c-4fd9-b49f-f5276b7795dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/transformed-data/dim_country/</td><td>dim_country/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/transformed-data/dim_date/</td><td>dim_date/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/transformed-data/fact_cases_death/</td><td>fact_cases_death/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/transformed-data/fact_response_country/</td><td>fact_response_country/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/transformed-data/dim_country/",
         "dim_country/",
         0,
         0
        ],
        [
         "dbfs:/mnt/transformed-data/dim_date/",
         "dim_date/",
         0,
         0
        ],
        [
         "dbfs:/mnt/transformed-data/fact_cases_death/",
         "fact_cases_death/",
         0,
         0
        ],
        [
         "dbfs:/mnt/transformed-data/fact_response_country/",
         "fact_response_country/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls \"/mnt/transformed-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee739b97-b83b-454b-b0cf-d92aea993e99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6302816264317883#setting/sparkui/1014-161028-rbnv7b6g/driver-2633383092808849376\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fba50625720>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbaad33c-4dbf-455f-b7ab-d8dc01d0ce2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read the data\n",
    "admissions_hospital = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/covid-data/admissions_hospital\")\n",
    "death_uk_ind_only = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/covid-data/death_uk_ind_only\")\n",
    "death_world = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/covid-data/death_world\")\n",
    "response_country = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/covid-data/response_country\")\n",
    "test = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/covid-data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09f80f2-b011-42c5-b244-f773f24a6fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|    country|country_code|continent|population|      indicator|daily_count|      date|rate_14_day|              source|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-02|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-03|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-04|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-05|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-06|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-07|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-08|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-09|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-10|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-11|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-12|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-13|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-14|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-15|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-16|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-17|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-18|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-19|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-20|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-21|          0|Epidemic intellig...|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "death_world.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc68a53-af2a-406d-9579-364b3de843d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- indicator: string (nullable = true)\n",
      " |-- daily_count: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- rate_14_day: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "death_world.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ceb182-0004-457a-8f9c-65d6ef30f891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- indicator: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year_week: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "admissions_hospital.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8140e913-91d3-4240-b74c-35420497dab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf7876d-4e80-46f2-872b-f5c014e6f599",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
      "| indic_de,geo\\time| 2008 |2009 | 2010 | 2011 | 2012 |2013 | 2014 | 2015 |2016 | 2017 | 2018 | 2019 |\n",
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
      "|       PC_Y0_14,AD| 14.6 |14.5 | 14.5 | 15.5 | 15.5 |15.5 |    : |    : |   : |    : |    : | 13.9 |\n",
      "|       PC_Y0_14,AL| 24.1 |23.3 | 22.5 | 21.6 | 20.7 |20.1 | 19.6 | 19.0 |18.5 | 18.2 | 17.7 | 17.2 |\n",
      "|       PC_Y0_14,AM| 19.0 |18.6 | 18.3 |    : |    : |   : |    : | 19.4 |19.6 | 20.0 | 20.2 | 20.2 |\n",
      "|       PC_Y0_14,AT| 15.4 |15.1 | 14.9 | 14.7 | 14.6 |14.4 | 14.3 | 14.3 |14.3 | 14.4 | 14.4 | 14.4 |\n",
      "|       PC_Y0_14,AZ| 23.2 |22.6 | 22.6 | 22.3 | 22.2 |22.3 | 22.4 | 22.4 |22.5 | 22.6 | 22.6 | 22.4 |\n",
      "|       PC_Y0_14,BE| 16.9 |16.9 | 16.9 |17.0 b| 17.0 |17.0 | 17.0 | 17.0 |17.0 | 17.0 | 17.0 | 16.9 |\n",
      "|       PC_Y0_14,BG| 13.1 |13.1 | 13.2 | 13.2 | 13.4 |13.6 | 13.7 | 13.9 |14.0 | 14.1 | 14.2 | 14.4 |\n",
      "|       PC_Y0_14,BY| 14.7 |14.6 |    : | 14.9 | 15.1 |15.4 | 15.7 | 16.0 |16.3 | 16.6 | 16.8 | 16.9 |\n",
      "|       PC_Y0_14,CH| 15.5 |15.3 | 15.2 |15.1 b| 15.0 |14.9 | 14.9 | 14.9 |14.9 | 14.9 | 15.0 | 15.0 |\n",
      "|       PC_Y0_14,CY| 18.2 |17.7 | 17.2 | 16.8 | 16.5 |16.4 | 16.3 | 16.4 |16.4 | 16.3 | 16.2 | 16.1 |\n",
      "|       PC_Y0_14,CZ| 14.2 |14.2 | 14.3 | 14.5 | 14.7 |14.8 | 15.0 | 15.2 |15.4 | 15.6 | 15.7 | 15.9 |\n",
      "|       PC_Y0_14,DE| 13.7 |13.6 | 13.5 |13.6 b| 13.4 |13.3 |13.2 b| 13.2 |13.2 | 13.4 | 13.5 | 13.6 |\n",
      "|       PC_Y0_14,DK| 18.4 |18.3 | 18.1 | 17.9 | 17.7 |17.4 | 17.2 | 17.0 |16.8 | 16.7 | 16.6 | 16.5 |\n",
      "|     PC_Y0_14,EA18| 15.4 |15.4 | 15.4 |15.4 b|15.4 b|15.3 |15.3 b|15.2 b|15.1 |15.1 b|15.0 p|15.0 p|\n",
      "|     PC_Y0_14,EA19| 15.5 |15.5 | 15.4 |15.5 b|15.4 b|15.4 |15.3 b|15.3 b|15.2 |15.2 b|15.1 p|15.0 p|\n",
      "|       PC_Y0_14,EE| 14.8 |14.9 | 15.1 | 15.3 | 15.5 |15.7 | 15.8 |15.9 b|16.1 | 16.2 | 16.3 | 16.4 |\n",
      "|       PC_Y0_14,EL| 14.6 |14.6 | 14.6 | 14.6 | 14.7 |14.7 | 14.6 | 14.5 |14.4 | 14.4 | 14.4 | 14.3 |\n",
      "|       PC_Y0_14,ES| 14.6 |14.8 | 14.9 | 15.0 | 15.1 |15.2 | 15.2 | 15.2 |15.1 | 15.1 | 15.0 | 14.8 |\n",
      "|PC_Y0_14,EU27_2007|15.8 b|15.7 |15.7 b|15.7 b|15.7 b|15.7 |15.6 b|15.6 b|15.6 |15.6 b|15.6 p|15.5 p|\n",
      "|PC_Y0_14,EU27_2020|15.5 b|15.4 |15.4 b|15.4 b|15.4 b|15.4 |15.3 b|15.3 b|15.3 |15.2 b|15.2 p|15.2 p|\n",
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the country code & age group\n",
    "population =  spark.read.csv(\"/mnt/raw-data/tps00010.tsv\", sep=r'\\t', header=True)\n",
    "population.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf9fb52-71cf-440c-aca9-3929d51a1e87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+---------+------------+\n",
      "| indic_de,geo\\time| 2008 |2009 | 2010 | 2011 | 2012 |2013 | 2014 | 2015 |2016 | 2017 | 2018 | 2019 |age_group|country_code|\n",
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+---------+------------+\n",
      "|       PC_Y0_14,AD| 14.6 |14.5 | 14.5 | 15.5 | 15.5 |15.5 |    : |    : |   : |    : |    : | 13.9 |    Y0_14|          AD|\n",
      "|       PC_Y0_14,AL| 24.1 |23.3 | 22.5 | 21.6 | 20.7 |20.1 | 19.6 | 19.0 |18.5 | 18.2 | 17.7 | 17.2 |    Y0_14|          AL|\n",
      "|       PC_Y0_14,AM| 19.0 |18.6 | 18.3 |    : |    : |   : |    : | 19.4 |19.6 | 20.0 | 20.2 | 20.2 |    Y0_14|          AM|\n",
      "|       PC_Y0_14,AT| 15.4 |15.1 | 14.9 | 14.7 | 14.6 |14.4 | 14.3 | 14.3 |14.3 | 14.4 | 14.4 | 14.4 |    Y0_14|          AT|\n",
      "|       PC_Y0_14,AZ| 23.2 |22.6 | 22.6 | 22.3 | 22.2 |22.3 | 22.4 | 22.4 |22.5 | 22.6 | 22.6 | 22.4 |    Y0_14|          AZ|\n",
      "|       PC_Y0_14,BE| 16.9 |16.9 | 16.9 |17.0 b| 17.0 |17.0 | 17.0 | 17.0 |17.0 | 17.0 | 17.0 | 16.9 |    Y0_14|          BE|\n",
      "|       PC_Y0_14,BG| 13.1 |13.1 | 13.2 | 13.2 | 13.4 |13.6 | 13.7 | 13.9 |14.0 | 14.1 | 14.2 | 14.4 |    Y0_14|          BG|\n",
      "|       PC_Y0_14,BY| 14.7 |14.6 |    : | 14.9 | 15.1 |15.4 | 15.7 | 16.0 |16.3 | 16.6 | 16.8 | 16.9 |    Y0_14|          BY|\n",
      "|       PC_Y0_14,CH| 15.5 |15.3 | 15.2 |15.1 b| 15.0 |14.9 | 14.9 | 14.9 |14.9 | 14.9 | 15.0 | 15.0 |    Y0_14|          CH|\n",
      "|       PC_Y0_14,CY| 18.2 |17.7 | 17.2 | 16.8 | 16.5 |16.4 | 16.3 | 16.4 |16.4 | 16.3 | 16.2 | 16.1 |    Y0_14|          CY|\n",
      "|       PC_Y0_14,CZ| 14.2 |14.2 | 14.3 | 14.5 | 14.7 |14.8 | 15.0 | 15.2 |15.4 | 15.6 | 15.7 | 15.9 |    Y0_14|          CZ|\n",
      "|       PC_Y0_14,DE| 13.7 |13.6 | 13.5 |13.6 b| 13.4 |13.3 |13.2 b| 13.2 |13.2 | 13.4 | 13.5 | 13.6 |    Y0_14|          DE|\n",
      "|       PC_Y0_14,DK| 18.4 |18.3 | 18.1 | 17.9 | 17.7 |17.4 | 17.2 | 17.0 |16.8 | 16.7 | 16.6 | 16.5 |    Y0_14|          DK|\n",
      "|     PC_Y0_14,EA18| 15.4 |15.4 | 15.4 |15.4 b|15.4 b|15.3 |15.3 b|15.2 b|15.1 |15.1 b|15.0 p|15.0 p|    Y0_14|        EA18|\n",
      "|     PC_Y0_14,EA19| 15.5 |15.5 | 15.4 |15.5 b|15.4 b|15.4 |15.3 b|15.3 b|15.2 |15.2 b|15.1 p|15.0 p|    Y0_14|        EA19|\n",
      "|       PC_Y0_14,EE| 14.8 |14.9 | 15.1 | 15.3 | 15.5 |15.7 | 15.8 |15.9 b|16.1 | 16.2 | 16.3 | 16.4 |    Y0_14|          EE|\n",
      "|       PC_Y0_14,EL| 14.6 |14.6 | 14.6 | 14.6 | 14.7 |14.7 | 14.6 | 14.5 |14.4 | 14.4 | 14.4 | 14.3 |    Y0_14|          EL|\n",
      "|       PC_Y0_14,ES| 14.6 |14.8 | 14.9 | 15.0 | 15.1 |15.2 | 15.2 | 15.2 |15.1 | 15.1 | 15.0 | 14.8 |    Y0_14|          ES|\n",
      "|PC_Y0_14,EU27_2007|15.8 b|15.7 |15.7 b|15.7 b|15.7 b|15.7 |15.6 b|15.6 b|15.6 |15.6 b|15.6 p|15.5 p|    Y0_14|   EU27_2007|\n",
      "|PC_Y0_14,EU27_2020|15.5 b|15.4 |15.4 b|15.4 b|15.4 b|15.4 |15.3 b|15.3 b|15.3 |15.2 b|15.2 p|15.2 p|    Y0_14|   EU27_2020|\n",
      "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population = population.withColumn('age_group', regexp_replace(split(population['indic_de,geo\\\\time'], ',')[0], 'PC_', '')).withColumn('country_code', split(population['indic_de,geo\\\\time'], ',')[1])\n",
    "population.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1fc494a-f280-43c8-9864-904cb22b2765",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+---------------+\n",
      "|country_code|age_group|percentage_2019|\n",
      "+------------+---------+---------------+\n",
      "|          AD|    Y0_14|          13.9 |\n",
      "|          AL|    Y0_14|          17.2 |\n",
      "|          AM|    Y0_14|          20.2 |\n",
      "|          AT|    Y0_14|          14.4 |\n",
      "|          AZ|    Y0_14|          22.4 |\n",
      "|          BE|    Y0_14|          16.9 |\n",
      "|          BG|    Y0_14|          14.4 |\n",
      "|          BY|    Y0_14|          16.9 |\n",
      "|          CH|    Y0_14|          15.0 |\n",
      "|          CY|    Y0_14|          16.1 |\n",
      "|          CZ|    Y0_14|          15.9 |\n",
      "|          DE|    Y0_14|          13.6 |\n",
      "|          DK|    Y0_14|          16.5 |\n",
      "|        EA18|    Y0_14|         15.0 p|\n",
      "|        EA19|    Y0_14|         15.0 p|\n",
      "|          EE|    Y0_14|          16.4 |\n",
      "|          EL|    Y0_14|          14.3 |\n",
      "|          ES|    Y0_14|          14.8 |\n",
      "|   EU27_2007|    Y0_14|         15.5 p|\n",
      "|   EU27_2020|    Y0_14|         15.2 p|\n",
      "+------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we want the data to Exclude all data other than 2019\n",
    "population = population.select(col(\"country_code\").alias(\"country_code\"),\n",
    "                               col(\"age_group\").alias(\"age_group\"),\n",
    "                               col(\"2019 \").alias(\"percentage_2019\"))\n",
    "population.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdd9079-a9d8-4f2a-8e94-2f55ef0eaf91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------+----------+\n",
      "|       country|country_code|continent|population|\n",
      "+--------------+------------+---------+----------+\n",
      "|       Algeria|         DZA|   Africa|  43851043|\n",
      "|       Andorra|         AND|   Europe|     76177|\n",
      "|        Angola|         AGO|   Africa|  32866268|\n",
      "|Africa (total)|        NULL|   Africa|1339423921|\n",
      "|       Albania|         ALB|   Europe|   2862427|\n",
      "|   Afghanistan|         AFG|     Asia|  38928341|\n",
      "+--------------+------------+---------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's process and transform the death world data\n",
    "death_world.createOrReplaceTempView(\"raw_death_world\") #to query the files\n",
    "dim_country = spark.sql(\"\"\"select distinct country, country_code, continent, population \n",
    "                                  from raw_death_world\n",
    "                                  \"\"\")\n",
    "dim_country.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54130404-6531-4c38-9a5f-befc3583e929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_country.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").mode(\"overwrite\").save(\"/mnt/transformed-data/dim_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae260936-ffd6-4dcf-bc0d-42ecb5603262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|    country|country_code|continent|population|      indicator|daily_count|      date|rate_14_day|              source|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-02|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-03|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-04|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-05|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-06|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-07|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-08|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-09|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-10|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-11|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-12|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-13|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-14|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-15|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-16|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-17|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-18|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-19|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-20|          0|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-21|          0|Epidemic intellig...|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "death_world.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2570058-004a-40db-b566-a6e79b4b43e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+-----+---+---------+-----------+-------------+------------+----------+----------+---------+\n",
      "|date_key|      date|year|month|day| day_name|day_of_year|week_of_month|week_of_year|month_name|year_month|year_week|\n",
      "+--------+----------+----+-----+---+---------+-----------+-------------+------------+----------+----------+---------+\n",
      "|20200101|2020-01-01|2020|    1|  1|Wednesday|          1|            1|           1|   January|    202001|   202001|\n",
      "|20200102|2020-01-02|2020|    1|  2| Thursday|          2|            1|           1|   January|    202001|   202001|\n",
      "|20200103|2020-01-03|2020|    1|  3|   Friday|          3|            1|           1|   January|    202001|   202001|\n",
      "|20200104|2020-01-04|2020|    1|  4| Saturday|          4|            1|           1|   January|    202001|   202001|\n",
      "|20200105|2020-01-05|2020|    1|  5|   Sunday|          5|            2|           2|   January|    202001|   202002|\n",
      "+--------+----------+----+-----+---+---------+-----------+-------------+------------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/mnt/raw-data/dim_date.csv\");\n",
    "\n",
    "dim_date.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d658aca-3c4c-4fad-8e38-54dd744f109c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_date.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").mode(\"overwrite\").save(\"/mnt/transformed-data/dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb92176-0904-4670-afda-fa6238e882af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|    country|country_code|continent|population|      indicator|daily_count|      date|rate_14_day|              source|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-02|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-03|       NULL|Epidemic intellig...|\n",
      "|Afghanistan|         AFG|     Asia|  38928341|confirmed cases|          0|2020-01-04|       NULL|Epidemic intellig...|\n",
      "+-----------+------------+---------+----------+---------------+-----------+----------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "death_world.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c61bb28-9781-498f-8bab-6f1c182d92f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------------+-----------+-----------+--------------------+\n",
      "|    country|date_key|      indicator|rate_14_day|daily_count|              source|\n",
      "+-----------+--------+---------------+-----------+-----------+--------------------+\n",
      "|Afghanistan|20200102|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "|Afghanistan|20200103|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "|Afghanistan|20200104|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "|Afghanistan|20200105|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "|Afghanistan|20200106|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "|Afghanistan|20200107|confirmed cases|       NULL|          0|Epidemic intellig...|\n",
      "+-----------+--------+---------------+-----------+-----------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's go now for the fact table of cases death\n",
    "\n",
    "death_world.createOrReplaceTempView(\"death_world\") #pour que on puisse faire des requetes\n",
    "fact_cases_death = spark.sql(\"\"\"select country, date_format(date,'yyyyMMdd' ) as date_key,\n",
    "                                indicator, rate_14_day, daily_count, source \n",
    "                                  from death_world\n",
    "                                  \"\"\")\n",
    "fact_cases_death.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a990495d-7cb6-4871-bb2d-b76835efd32b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_cases_death.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").mode(\"overwrite\").save(\"/mnt/transformed-data/fact_cases_death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf191f0-0387-47af-a8df-8251df8af6a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+----------+----------+\n",
      "|Country|    Response_measure|change|date_start|  date_end|\n",
      "+-------+--------------------+------+----------+----------+\n",
      "|Austria|AdaptationOfWorkp...|     1|2020-03-10|        NA|\n",
      "|Austria|         ClosDaycare|     1|2020-03-16|2020-05-04|\n",
      "|Austria|            ClosHigh|     1|2020-03-16|2020-09-30|\n",
      "|Austria|            ClosPrim|     1|2020-03-16|2020-05-18|\n",
      "|Austria|          ClosPubAny|     1|2020-03-16|2020-04-13|\n",
      "+-------+--------------------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's go for fact table country response\n",
    "response_country.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aabacc5-6191-4b67-b23d-82c7e434fa9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+----------+--------+\n",
      "|Country|    Response_measure|change|date_start|date_end|\n",
      "+-------+--------------------+------+----------+--------+\n",
      "|Austria|AdaptationOfWorkp...|     1|  20200310|    NULL|\n",
      "|Austria|         ClosDaycare|     1|  20200316|20200504|\n",
      "|Austria|            ClosHigh|     1|  20200316|20200930|\n",
      "+-------+--------------------+------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_country.createOrReplaceTempView(\"response_country\")\n",
    "fact_response_country = spark.sql(\"\"\"select Country, Response_measure, change, date_format(date_start,'yyyyMMdd' ) as date_start\n",
    "                                  ,date_format(date_end,'yyyyMMdd' ) as date_end\n",
    "                                  from response_country\"\"\")\n",
    "fact_response_country.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2a923d-9a54-457d-8dd3-72e0702d50a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_response_country.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").mode(\"overwrite\").save(\"/mnt/transformed-data/fact_response_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95951e76-1a3d-40e2-8334-314767949b81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+-----+------------+--------------------+\n",
      "|country|           indicator|      date|year_week|value|      source|                 url|\n",
      "+-------+--------------------+----------+---------+-----+------------+--------------------+\n",
      "|Austria|Daily hospital oc...|2020-04-02| 2020-W14| 1057|Surveillance|https://www.sozia...|\n",
      "|Austria|Daily hospital oc...|2020-04-08| 2020-W15| 1096|Surveillance|https://www.sozia...|\n",
      "|Austria|Daily hospital oc...|2020-04-15| 2020-W16| 1001|Surveillance|https://info.gesu...|\n",
      "+-------+--------------------+----------+---------+-----+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#admissions hospital\n",
    "admissions_hospital.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a4d89b-455e-4e42-9540-12b2e3357906",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+-----+------------+--------------------+\n",
      "|country|           indicator|date_key|value|      source|                 url|\n",
      "+-------+--------------------+--------+-----+------------+--------------------+\n",
      "|Austria|Daily hospital oc...|20200402| 1057|Surveillance|https://www.sozia...|\n",
      "|Austria|Daily hospital oc...|20200408| 1096|Surveillance|https://www.sozia...|\n",
      "|Austria|Daily hospital oc...|20200415| 1001|Surveillance|https://info.gesu...|\n",
      "|Austria|Daily hospital oc...|20200416|  967|Surveillance|https://www.sozia...|\n",
      "+-------+--------------------+--------+-----+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "admissions_hospital.createOrReplaceTempView(\"admissions_hospital\")\n",
    "fact_admissions_hosptal = spark.sql(\"\"\"select country\n",
    "                                    , indicator\n",
    "                                    , date_format(date,'yyyyMMdd' ) AS date_key\n",
    "                                    , value \n",
    "                                    ,source\n",
    "                                    ,url \n",
    "                                    from admissions_hospital  \"\"\")\n",
    "fact_admissions_hosptal.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819551e3-1693-4eab-8a65-8a42abcd4a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_admissions_hosptal.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").mode(\"overwrite\").save(\"/mnt/transformed-data/fact_admissions_hosptal\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1146151348960861,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook Covid-db",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
